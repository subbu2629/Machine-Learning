{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import treebank\n",
    "from nltk.grammar import PCFG, induce_pcfg, toy_pcfg1, toy_pcfg2\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# import re\n",
    "# from bs4 import BeautifulSoup #To remove HTML tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step#1: Read and store all positive reviews in a list called ‘pos’ and all negative reviews in a list called ‘neg’. \n",
    "Create a Dataframe with all reviews under column ‘Review’, add a new column ‘class’ assigning label '1' for \n",
    "positive reviews and '0' for negative reviews (4 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I saw this movie when I was about 12 when it c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>So im not a big fan of Boll's work but then ag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The cast played Shakespeare.&lt;br /&gt;&lt;br /&gt;Shakes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kind of drawn in by the erotic scenes, only to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Some films just simply should not be remade. T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>This movie made it into one of my top 10 most ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I remember this film,it was the first film i h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>An awful film! It must have been up against so...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review  class\n",
       "0   One of the other reviewers has mentioned that ...      1\n",
       "1   A wonderful little production. <br /><br />The...      1\n",
       "2   I thought this was a wonderful way to spend ti...      1\n",
       "3   Basically there's a family where a little boy ...      0\n",
       "4   Petter Mattei's \"Love in the Time of Money\" is...      1\n",
       "5   Probably my all-time favorite movie, a story o...      1\n",
       "6   I sure would like to see a resurrection of a u...      1\n",
       "7   This show was an amazing, fresh & innovative i...      0\n",
       "8   Encouraged by the positive comments about this...      0\n",
       "9   If you like original gut wrenching laughter yo...      1\n",
       "10  Phil the Alien is one of those quirky films wh...      0\n",
       "11  I saw this movie when I was about 12 when it c...      0\n",
       "12  So im not a big fan of Boll's work but then ag...      0\n",
       "13  The cast played Shakespeare.<br /><br />Shakes...      0\n",
       "14  This a fantastic movie of three prisoners who ...      1\n",
       "15  Kind of drawn in by the erotic scenes, only to...      0\n",
       "16  Some films just simply should not be remade. T...      1\n",
       "17  This movie made it into one of my top 10 most ...      0\n",
       "18  I remember this film,it was the first film i h...      1\n",
       "19  An awful film! It must have been up against so...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To read and store all positive reviews in pos and neg separately\n",
    "df = pd.read_csv('D://M.Tech//Sem 3//Natural Language processing//imdb-dataset-of-50k-movie-reviews//IMDB Dataset.csv')\n",
    "# df.head()\n",
    "pos=df[\"review\"][df[\"sentiment\"]=='positive']\n",
    "pos.tolist()\n",
    "neg=df[\"review\"][df[\"sentiment\"]=='negative']\n",
    "neg.tolist()\n",
    "lbl = {\"positive\": 1, \"negative\": 0}\n",
    "dataset=pd.DataFrame(df).rename(columns={'review': 'Review', 'sentiment': 'class'}).replace({\"class\": lbl}) \n",
    "dataset.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              class\n",
       "count  50000.000000\n",
       "mean       0.500000\n",
       "std        0.500005\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.500000\n",
       "75%        1.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step#2:Remove punctuations and stopwords from the text in ‘Review’ column (4 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\subba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(\"<.*?>\", \"\",text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "dataset['Processed_Reviews'] = dataset.Review.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>class</th>\n",
       "      <th>Processed_Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>one reviewer ha mention watch 1 oz episode you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "      <td>wonderful little production film technique una...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>think wa wonderful way spend time hot summer w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "      <td>petter matteis love time money visually stun f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  class  \\\n",
       "0  One of the other reviewers has mentioned that ...      1   \n",
       "1  A wonderful little production. <br /><br />The...      1   \n",
       "2  I thought this was a wonderful way to spend ti...      1   \n",
       "3  Basically there's a family where a little boy ...      0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...      1   \n",
       "\n",
       "                                   Processed_Reviews  \n",
       "0  one reviewer ha mention watch 1 oz episode you...  \n",
       "1  wonderful little production film technique una...  \n",
       "2  think wa wonderful way spend time hot summer w...  \n",
       "3  basically family little boy jake think zombie ...  \n",
       "4  petter matteis love time money visually stun f...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step#3:Create two objects X and y. X will be the ' Review' column of the above dataframe and y will be the 'class' column.\n",
    "Create a CountVectorizer object and split the data into training and testing sets. Train a MultinomialNB model for \n",
    "classifying the reviews and Display the confusion Matrix (5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Creating the Bag of Words model\n",
    "cv = CountVectorizer(max_features = 2000)\n",
    "X = cv.fit_transform(dataset['Processed_Reviews']).toarray()\n",
    "dataset\n",
    "y = dataset.iloc[0:50000, 1].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4203,  832],\n",
       "       [ 838, 4127]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=clf.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=(4203+4127)/(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.833\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step#4:Display the HMM POS tagging on the first 4 rows of ‘Review’ (3 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    one reviewer ha mention watch 1 oz episode youll hook right exactly happen methe first thing strike oz wa brutality unflinching scene violence set right word go trust show faint hearted timid show pull punch regard drug sex violence hardcore classic use wordit call oz nickname give oswald maximum security state penitentary focus mainly emerald city experimental section prison cell glass front face inwards privacy high agenda em city home manyaryans muslim gangsta latino christian italian irish moreso scuffle death stare dodgy deal shady agreement never far awayi would say main appeal show due fact go show wouldnt dare. forget pretty picture paint mainstream audiences, forget charm, forget romance...oz mess around. first episode ever saw strike nasty wa surreal, say wa ready it, watch more, develop taste oz, get accustom high level graphic violence. violence, injustice (crooked guard who'll sell nickel, inmate who'll kill order get away it, well mannered, middle class inmate turn prison bitch due lack street skill prison experience) watch oz, may become comfortable uncomfortable viewing....thats get touch darker side.\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         wonderful little production film technique unassuming oldtimebbc fashion give comfort sometimes discomforting sense realism entire piece actor extremely well choose michael sheen ha get polari ha voice pat truly see seamless edit guide reference williams diary entry well worth watch terrificly write perform piece masterful production one great master comedy life realism really come home little thing fantasy guard rather use traditional dream technique remain solid disappear play knowledge sens particularly scene concern orton halliwell set particularly flat halliwells mural decorate every surface terribly well\n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              think wa wonderful way spend time hot summer weekend sit air condition theater watch lighthearted comedy plot simplistic dialogue witty character likable even well bread suspect serial killer may disappoint realize match point 2 risk addiction think wa proof woody allen still fully control style many u grow lovethis wa id laugh one woodys comedy year dare say decade ive never impress scarlet johanson manage tone sexy image jump right average spirit young womanthis may crown jewel career wa wittier devil wear prada interest superman great comedy go see friend\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         basically family little boy jake think zombie closet  parent fight timethis movie slower soap opera suddenly jake decide become rambo kill zombieok first youre go make film must decide thriller drama drama movie watchable parent divorce  argue like real life jake closet totally ruin film expect see boogeyman similar movie instead watch drama meaningless thriller spots3 10 well play parent  descent dialog shoot jake ignore\n",
       "Name: Processed_Reviews, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 2000)\n",
    "dataset[\"Processed_Reviews\"].iloc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\subba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\subba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts of speech for first four lines is: \n",
      " \n",
      " [('one', 'CD'), ('reviewer', 'JJR'), ('ha', 'NN'), ('mention', 'NN'), ('watch', 'NN'), ('1', 'CD'), ('oz', 'JJ'), ('episode', 'NN'), ('youll', 'NN'), ('hook', 'NN'), ('right', 'RB'), ('exactly', 'RB'), ('happen', 'VB'), ('methe', 'NN'), ('first', 'JJ'), ('thing', 'NN'), ('strike', 'NN'), ('oz', 'VBP'), ('wa', 'JJ'), ('brutality', 'NN'), ('unflinching', 'VBG'), ('scene', 'NN'), ('violence', 'NN'), ('set', 'VBN'), ('right', 'RB'), ('word', 'NN'), ('go', 'VB'), ('trust', 'NN'), ('show', 'NN'), ('faint', 'NN'), ('hearted', 'VBD'), ('timid', 'JJ'), ('show', 'NN'), ('pull', 'JJ'), ('punch', 'JJ'), ('regard', 'JJ'), ('drug', 'NN'), ('sex', 'NN'), ('violence', 'NN'), ('hardcore', 'NN'), ('classic', 'JJ'), ('use', 'NN'), ('wordit', 'NN'), ('call', 'NN'), ('oz', 'NN'), ('nickname', 'NN'), ('give', 'JJ'), ('oswald', 'NN'), ('maximum', 'JJ'), ('security', 'NN'), ('state', 'NN'), ('penitentary', 'JJ'), ('focus', 'NN'), ('mainly', 'RB'), ('emerald', 'VBZ'), ('city', 'NN'), ('experimental', 'JJ'), ('section', 'NN'), ('prison', 'NN'), ('cell', 'NN'), ('glass', 'NN'), ('front', 'JJ'), ('face', 'NN'), ('inwards', 'NNS'), ('privacy', 'VBP'), ('high', 'JJ'), ('agenda', 'NN'), ('em', 'VBP'), ('city', 'NN'), ('home', 'NN'), ('manyaryans', 'NNS'), ('muslim', 'VBP'), ('gangsta', 'JJ'), ('latino', 'NN'), ('christian', 'JJ'), ('italian', 'JJ'), ('irish', 'JJ'), ('moreso', 'NN'), ('scuffle', 'VBD'), ('death', 'NN'), ('stare', 'NN'), ('dodgy', 'JJ'), ('deal', 'NN'), ('shady', 'JJ'), ('agreement', 'NN'), ('never', 'RB'), ('far', 'RB'), ('awayi', 'JJ'), ('would', 'MD'), ('say', 'VB'), ('main', 'JJ'), ('appeal', 'NN'), ('show', 'NN'), ('due', 'JJ'), ('fact', 'NN'), ('go', 'VB'), ('show', 'NN'), ('wouldnt', 'NN'), ('dare', 'NN'), ('.', '.'), ('forget', 'VB'), ('pretty', 'JJ'), ('picture', 'NN'), ('paint', 'NN'), ('mainstream', 'NN'), ('audiences', 'NNS'), (',', ','), ('forget', 'VB'), ('charm', 'NN'), (',', ','), ('forget', 'VB'), ('romance', 'NN'), ('...', ':'), ('oz', 'JJ'), ('mess', 'NN'), ('around', 'IN'), ('.', '.'), ('first', 'JJ'), ('episode', 'NN'), ('ever', 'RB'), ('saw', 'VBD'), ('strike', 'NN'), ('nasty', 'JJ'), ('wa', 'NN'), ('surreal', 'NN'), (',', ','), ('say', 'VBP'), ('wa', 'UH'), ('ready', 'JJ'), ('it', 'PRP'), (',', ','), ('watch', 'VB'), ('more', 'JJR'), (',', ','), ('develop', 'VB'), ('taste', 'NN'), ('oz', 'NN'), (',', ','), ('get', 'VB'), ('accustom', 'JJ'), ('high', 'JJ'), ('level', 'NN'), ('graphic', 'JJ'), ('violence', 'NN'), ('.', '.'), ('violence', 'NN'), (',', ','), ('injustice', 'NN'), ('(', '('), ('crooked', 'VBN'), ('guard', 'NN'), ('who', 'WP'), (\"'ll\", 'MD'), ('sell', 'VB'), ('nickel', 'RB'), (',', ','), ('inmate', 'NN'), ('who', 'WP'), (\"'ll\", 'MD'), ('kill', 'VB'), ('order', 'NN'), ('get', 'VB'), ('away', 'RB'), ('it', 'PRP'), (',', ','), ('well', 'RB'), ('mannered', 'VBN'), (',', ','), ('middle', 'JJ'), ('class', 'NN'), ('inmate', 'NN'), ('turn', 'NN'), ('prison', 'NN'), ('bitch', 'NN'), ('due', 'JJ'), ('lack', 'NN'), ('street', 'NN'), ('skill', 'JJ'), ('prison', 'NN'), ('experience', 'NN'), (')', ')'), ('watch', 'NN'), ('oz', 'NN'), (',', ','), ('may', 'MD'), ('become', 'VB'), ('comfortable', 'JJ'), ('uncomfortable', 'JJ'), ('viewing', 'NN'), ('...', ':'), ('.thats', 'VBZ'), ('get', 'VBP'), ('touch', 'JJ'), ('darker', 'NN'), ('side', 'NN'), ('.', '.'), ('wonderful', 'JJ'), ('little', 'JJ'), ('production', 'NN'), ('film', 'NN'), ('technique', 'NN'), ('unassuming', 'VBG'), ('oldtimebbc', 'JJ'), ('fashion', 'NN'), ('give', 'VB'), ('comfort', 'NN'), ('sometimes', 'RB'), ('discomforting', 'VBG'), ('sense', 'NN'), ('realism', 'NN'), ('entire', 'JJ'), ('piece', 'NN'), ('actor', 'NN'), ('extremely', 'RB'), ('well', 'RB'), ('choose', 'VB'), ('michael', 'NNS'), ('sheen', 'VBP'), ('ha', 'JJ'), ('get', 'NN'), ('polari', 'JJ'), ('ha', 'JJ'), ('voice', 'NN'), ('pat', 'NN'), ('truly', 'RB'), ('see', 'VB'), ('seamless', 'JJ'), ('edit', 'NN'), ('guide', 'NN'), ('reference', 'NN'), ('williams', 'NNS'), ('diary', 'JJ'), ('entry', 'NN'), ('well', 'RB'), ('worth', 'JJ'), ('watch', 'NN'), ('terrificly', 'NN'), ('write', 'VB'), ('perform', 'JJ'), ('piece', 'NN'), ('masterful', 'JJ'), ('production', 'NN'), ('one', 'CD'), ('great', 'JJ'), ('master', 'NN'), ('comedy', 'NN'), ('life', 'NN'), ('realism', 'NN'), ('really', 'RB'), ('come', 'VBN'), ('home', 'NN'), ('little', 'JJ'), ('thing', 'NN'), ('fantasy', 'JJ'), ('guard', 'NN'), ('rather', 'RB'), ('use', 'JJ'), ('traditional', 'JJ'), ('dream', 'NN'), ('technique', 'NN'), ('remain', 'VBP'), ('solid', 'JJ'), ('disappear', 'JJ'), ('play', 'NN'), ('knowledge', 'NN'), ('sens', 'VBZ'), ('particularly', 'RB'), ('scene', 'JJ'), ('concern', 'NN'), ('orton', 'NN'), ('halliwell', 'NN'), ('set', 'VBN'), ('particularly', 'RB'), ('flat', 'JJ'), ('halliwells', 'NNS'), ('mural', 'JJ'), ('decorate', 'NN'), ('every', 'DT'), ('surface', 'NN'), ('terribly', 'RB'), ('well', 'RB'), ('think', 'VB'), ('wa', 'NNS'), ('wonderful', 'JJ'), ('way', 'NN'), ('spend', 'JJ'), ('time', 'NN'), ('hot', 'JJ'), ('summer', 'NN'), ('weekend', 'NN'), ('sit', 'NN'), ('air', 'NN'), ('condition', 'NN'), ('theater', 'NN'), ('watch', 'NN'), ('lighthearted', 'VBD'), ('comedy', 'NN'), ('plot', 'NN'), ('simplistic', 'JJ'), ('dialogue', 'NN'), ('witty', 'JJ'), ('character', 'NN'), ('likable', 'JJ'), ('even', 'RB'), ('well', 'RB'), ('bread', 'JJ'), ('suspect', 'VBP'), ('serial', 'JJ'), ('killer', 'NN'), ('may', 'MD'), ('disappoint', 'VB'), ('realize', 'VB'), ('match', 'NN'), ('point', 'NN'), ('2', 'CD'), ('risk', 'NN'), ('addiction', 'NN'), ('think', 'VBP'), ('wa', 'IN'), ('proof', 'NN'), ('woody', 'NN'), ('allen', 'NN'), ('still', 'RB'), ('fully', 'RB'), ('control', 'VB'), ('style', 'NN'), ('many', 'JJ'), ('u', 'JJ'), ('grow', 'VB'), ('lovethis', 'JJ'), ('wa', 'NN'), ('id', 'NN'), ('laugh', 'IN'), ('one', 'CD'), ('woodys', 'NN'), ('comedy', 'NN'), ('year', 'NN'), ('dare', 'NN'), ('say', 'VBP'), ('decade', 'NN'), ('ive', 'JJ'), ('never', 'RB'), ('impress', 'JJ'), ('scarlet', 'NN'), ('johanson', 'NN'), ('manage', 'NN'), ('tone', 'NN'), ('sexy', 'JJ'), ('image', 'NN'), ('jump', 'NN'), ('right', 'JJ'), ('average', 'NN'), ('spirit', 'NN'), ('young', 'JJ'), ('womanthis', 'NN'), ('may', 'MD'), ('crown', 'VB'), ('jewel', 'NN'), ('career', 'NN'), ('wa', 'NN'), ('wittier', 'JJR'), ('devil', 'JJ'), ('wear', 'NN'), ('prada', 'JJ'), ('interest', 'NN'), ('superman', 'NN'), ('great', 'JJ'), ('comedy', 'NN'), ('go', 'VBP'), ('see', 'VB'), ('friend', 'RB'), ('basically', 'RB'), ('family', 'NN'), ('little', 'JJ'), ('boy', 'JJ'), ('jake', 'NN'), ('think', 'VBP'), ('zombie', 'NN'), ('closet', 'NN'), ('parent', 'NN'), ('fight', 'VBD'), ('timethis', 'JJ'), ('movie', 'NN'), ('slower', 'JJR'), ('soap', 'NN'), ('opera', 'NN'), ('suddenly', 'RB'), ('jake', 'VBZ'), ('decide', 'JJ'), ('become', 'JJ'), ('rambo', 'NN'), ('kill', 'VB'), ('zombieok', 'NNP'), ('first', 'JJ'), ('youre', 'NN'), ('go', 'VB'), ('make', 'JJ'), ('film', 'NN'), ('must', 'MD'), ('decide', 'VB'), ('thriller', 'NN'), ('drama', 'NN'), ('drama', 'NN'), ('movie', 'NN'), ('watchable', 'JJ'), ('parent', 'NN'), ('divorce', 'NN'), ('argue', 'NN'), ('like', 'IN'), ('real', 'JJ'), ('life', 'NN'), ('jake', 'NN'), ('closet', 'NN'), ('totally', 'RB'), ('ruin', 'JJ'), ('film', 'NN'), ('expect', 'VBP'), ('see', 'VB'), ('boogeyman', 'JJ'), ('similar', 'JJ'), ('movie', 'NN'), ('instead', 'RB'), ('watch', 'JJ'), ('drama', 'NN'), ('meaningless', 'NN'), ('thriller', 'NN'), ('spots3', 'NN'), ('10', 'CD'), ('well', 'RB'), ('play', 'VB'), ('parent', 'NN'), ('descent', 'NN'), ('dialog', 'NN'), ('shoot', 'VBP'), ('jake', 'NN'), ('ignore', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "fourrows=dataset[\"Processed_Reviews\"].iloc[:4].to_string(index=False)\n",
    "# print(fourrows)\n",
    "tokens=nltk.word_tokenize(fourrows)\n",
    "\n",
    "print(\"Parts of speech for first four lines is: \\n \\n\",  nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step#5:Parse the first 4 rows of ‘Review’ using Viterbi Parser \n",
    "[Use toy_pcfg1 and toy_pcfg2 to get the probabilistic context free grammars; use the PCFG suitable for each sentence] (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar=toy_pcfg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 23 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    VP -> V NP [0.59]\n",
      "    VP -> V [0.4]\n",
      "    VP -> VP PP [0.01]\n",
      "    NP -> Det N [0.41]\n",
      "    NP -> Name [0.28]\n",
      "    NP -> NP PP [0.31]\n",
      "    PP -> P NP [1.0]\n",
      "    V -> 'saw' [0.21]\n",
      "    V -> 'ate' [0.51]\n",
      "    V -> 'ran' [0.28]\n",
      "    N -> 'boy' [0.11]\n",
      "    N -> 'cookie' [0.12]\n",
      "    N -> 'table' [0.13]\n",
      "    N -> 'telescope' [0.14]\n",
      "    N -> 'hill' [0.5]\n",
      "    Name -> 'Jack' [0.52]\n",
      "    Name -> 'Bob' [0.48]\n",
      "    P -> 'with' [0.61]\n",
      "    P -> 'under' [0.39]\n",
      "    Det -> 'the' [0.41]\n",
      "    Det -> 'a' [0.31]\n",
      "    Det -> 'my' [0.28]\n"
     ]
    }
   ],
   "source": [
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcfg1 =\"I saw john with telescope\"\n",
    "# \"John saw my man under telescope\"\n",
    "# \"The man saw john with telescope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcfg2=\"the boy saw a cookie under my table\"\n",
    "# \"the boy saw hill with telescope\"\n",
    "# \"the boy saw Jack with Bob under the table with a telescope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sentence: I saw the man with my telescope\n",
      "parser: <ViterbiParser for <Grammar with 17 productions>>\n",
      "grammar: Grammar with 17 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    NP -> Det N [0.5]\n",
      "    NP -> NP PP [0.25]\n",
      "    NP -> 'John' [0.1]\n",
      "    NP -> 'I' [0.15]\n",
      "    Det -> 'the' [0.8]\n",
      "    Det -> 'my' [0.2]\n",
      "    N -> 'man' [0.5]\n",
      "    N -> 'telescope' [0.5]\n",
      "    VP -> VP PP [0.1]\n",
      "    VP -> V NP [0.7]\n",
      "    VP -> V [0.2]\n",
      "    V -> 'ate' [0.35]\n",
      "    V -> 'saw' [0.65]\n",
      "    PP -> P NP [1.0]\n",
      "    P -> 'with' [0.61]\n",
      "    P -> 'under' [0.39]\n",
      "Inserting tokens into the most likely constituents table...\n",
      "   Insert: |=......| I\n",
      "   Insert: |.=.....| saw\n",
      "   Insert: |..=....| the\n",
      "   Insert: |...=...| man\n",
      "   Insert: |....=..| with\n",
      "   Insert: |.....=.| my\n",
      "   Insert: |......=| telescope\n",
      "Finding the most likely constituents spanning 1 text elements...\n",
      "   Insert: |=......| NP -> 'I' [0.15]               0.1500000000 \n",
      "   Insert: |.=.....| V -> 'saw' [0.65]              0.6500000000 \n",
      "   Insert: |.=.....| VP -> V [0.2]                  0.1300000000 \n",
      "   Insert: |..=....| Det -> 'the' [0.8]             0.8000000000 \n",
      "   Insert: |...=...| N -> 'man' [0.5]               0.5000000000 \n",
      "   Insert: |....=..| P -> 'with' [0.61]             0.6100000000 \n",
      "   Insert: |.....=.| Det -> 'my' [0.2]              0.2000000000 \n",
      "   Insert: |......=| N -> 'telescope' [0.5]         0.5000000000 \n",
      "Finding the most likely constituents spanning 2 text elements...\n",
      "   Insert: |==.....| S -> NP VP [1.0]               0.0195000000 \n",
      "   Insert: |..==...| NP -> Det N [0.5]              0.2000000000 \n",
      "   Insert: |.....==| NP -> Det N [0.5]              0.0500000000 \n",
      "Finding the most likely constituents spanning 3 text elements...\n",
      "   Insert: |.===...| VP -> V NP [0.7]               0.0910000000 \n",
      "   Insert: |....===| PP -> P NP [1.0]               0.0305000000 \n",
      "Finding the most likely constituents spanning 4 text elements...\n",
      "   Insert: |====...| S -> NP VP [1.0]               0.0136500000 \n",
      "Finding the most likely constituents spanning 5 text elements...\n",
      "   Insert: |..=====| NP -> NP PP [0.25]             0.0015250000 \n",
      "Finding the most likely constituents spanning 6 text elements...\n",
      "   Insert: |.======| VP -> VP PP [0.1]              0.0002775500 \n",
      "   Insert: |.======| VP -> V NP [0.7]               0.0006938750 \n",
      "  Discard: |.======| VP -> VP PP [0.1]              0.0002775500 \n",
      "Finding the most likely constituents spanning 7 text elements...\n",
      "   Insert: |=======| S -> NP VP [1.0]               0.0001040812 \n",
      "\n",
      "sentence: John saw my man under telescope\n",
      "parser: <ViterbiParser for <Grammar with 17 productions>>\n",
      "grammar: Grammar with 17 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    NP -> Det N [0.5]\n",
      "    NP -> NP PP [0.25]\n",
      "    NP -> 'John' [0.1]\n",
      "    NP -> 'I' [0.15]\n",
      "    Det -> 'the' [0.8]\n",
      "    Det -> 'my' [0.2]\n",
      "    N -> 'man' [0.5]\n",
      "    N -> 'telescope' [0.5]\n",
      "    VP -> VP PP [0.1]\n",
      "    VP -> V NP [0.7]\n",
      "    VP -> V [0.2]\n",
      "    V -> 'ate' [0.35]\n",
      "    V -> 'saw' [0.65]\n",
      "    PP -> P NP [1.0]\n",
      "    P -> 'with' [0.61]\n",
      "    P -> 'under' [0.39]\n",
      "Inserting tokens into the most likely constituents table...\n",
      "   Insert: |=.....| John\n",
      "   Insert: |.=....| saw\n",
      "   Insert: |..=...| my\n",
      "   Insert: |...=..| man\n",
      "   Insert: |....=.| under\n",
      "   Insert: |.....=| telescope\n",
      "Finding the most likely constituents spanning 1 text elements...\n",
      "   Insert: |=.....| NP -> 'John' [0.1]              0.1000000000 \n",
      "   Insert: |.=....| V -> 'saw' [0.65]               0.6500000000 \n",
      "   Insert: |.=....| VP -> V [0.2]                   0.1300000000 \n",
      "   Insert: |..=...| Det -> 'my' [0.2]               0.2000000000 \n",
      "   Insert: |...=..| N -> 'man' [0.5]                0.5000000000 \n",
      "   Insert: |....=.| P -> 'under' [0.39]             0.3900000000 \n",
      "   Insert: |.....=| N -> 'telescope' [0.5]          0.5000000000 \n",
      "Finding the most likely constituents spanning 2 text elements...\n",
      "   Insert: |==....| S -> NP VP [1.0]                0.0130000000 \n",
      "   Insert: |..==..| NP -> Det N [0.5]               0.0500000000 \n",
      "Finding the most likely constituents spanning 3 text elements...\n",
      "   Insert: |.===..| VP -> V NP [0.7]                0.0227500000 \n",
      "Finding the most likely constituents spanning 4 text elements...\n",
      "   Insert: |====..| S -> NP VP [1.0]                0.0022750000 \n",
      "Finding the most likely constituents spanning 5 text elements...\n",
      "Finding the most likely constituents spanning 6 text elements...\n",
      "\n",
      "sentence:  man saw with my telescope\n",
      "parser: <ViterbiParser for <Grammar with 17 productions>>\n",
      "grammar: Grammar with 17 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    NP -> Det N [0.5]\n",
      "    NP -> NP PP [0.25]\n",
      "    NP -> 'John' [0.1]\n",
      "    NP -> 'I' [0.15]\n",
      "    Det -> 'the' [0.8]\n",
      "    Det -> 'my' [0.2]\n",
      "    N -> 'man' [0.5]\n",
      "    N -> 'telescope' [0.5]\n",
      "    VP -> VP PP [0.1]\n",
      "    VP -> V NP [0.7]\n",
      "    VP -> V [0.2]\n",
      "    V -> 'ate' [0.35]\n",
      "    V -> 'saw' [0.65]\n",
      "    PP -> P NP [1.0]\n",
      "    P -> 'with' [0.61]\n",
      "    P -> 'under' [0.39]\n",
      "Inserting tokens into the most likely constituents table...\n",
      "   Insert: |=....| man\n",
      "   Insert: |.=...| saw\n",
      "   Insert: |..=..| with\n",
      "   Insert: |...=.| my\n",
      "   Insert: |....=| telescope\n",
      "Finding the most likely constituents spanning 1 text elements...\n",
      "   Insert: |=....| N -> 'man' [0.5]                 0.5000000000 \n",
      "   Insert: |.=...| V -> 'saw' [0.65]                0.6500000000 \n",
      "   Insert: |.=...| VP -> V [0.2]                    0.1300000000 \n",
      "   Insert: |..=..| P -> 'with' [0.61]               0.6100000000 \n",
      "   Insert: |...=.| Det -> 'my' [0.2]                0.2000000000 \n",
      "   Insert: |....=| N -> 'telescope' [0.5]           0.5000000000 \n",
      "Finding the most likely constituents spanning 2 text elements...\n",
      "   Insert: |...==| NP -> Det N [0.5]                0.0500000000 \n",
      "Finding the most likely constituents spanning 3 text elements...\n",
      "   Insert: |..===| PP -> P NP [1.0]                 0.0305000000 \n",
      "Finding the most likely constituents spanning 4 text elements...\n",
      "   Insert: |.====| VP -> VP PP [0.1]                0.0003965000 \n",
      "Finding the most likely constituents spanning 5 text elements...\n",
      "\n",
      "sentence: the boy saw Jack with Bob under the table with a telescope\n",
      "parser: <ViterbiParser for <Grammar with 23 productions>>\n",
      "grammar: Grammar with 23 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    VP -> V NP [0.59]\n",
      "    VP -> V [0.4]\n",
      "    VP -> VP PP [0.01]\n",
      "    NP -> Det N [0.41]\n",
      "    NP -> Name [0.28]\n",
      "    NP -> NP PP [0.31]\n",
      "    PP -> P NP [1.0]\n",
      "    V -> 'saw' [0.21]\n",
      "    V -> 'ate' [0.51]\n",
      "    V -> 'ran' [0.28]\n",
      "    N -> 'boy' [0.11]\n",
      "    N -> 'cookie' [0.12]\n",
      "    N -> 'table' [0.13]\n",
      "    N -> 'telescope' [0.14]\n",
      "    N -> 'hill' [0.5]\n",
      "    Name -> 'Jack' [0.52]\n",
      "    Name -> 'Bob' [0.48]\n",
      "    P -> 'with' [0.61]\n",
      "    P -> 'under' [0.39]\n",
      "    Det -> 'the' [0.41]\n",
      "    Det -> 'a' [0.31]\n",
      "    Det -> 'my' [0.28]\n",
      "Inserting tokens into the most likely constituents table...\n",
      "   Insert: |=...........| the\n",
      "   Insert: |.=..........| boy\n",
      "   Insert: |..=.........| saw\n",
      "   Insert: |...=........| Jack\n",
      "   Insert: |....=.......| with\n",
      "   Insert: |.....=......| Bob\n",
      "   Insert: |......=.....| under\n",
      "   Insert: |.......=....| the\n",
      "   Insert: |........=...| table\n",
      "   Insert: |.........=..| with\n",
      "   Insert: |..........=.| a\n",
      "   Insert: |...........=| telescope\n",
      "Finding the most likely constituents spanning 1 text elements...\n",
      "   Insert: |=...........| Det -> 'the' [0.41]       0.4100000000 \n",
      "   Insert: |.=..........| N -> 'boy' [0.11]         0.1100000000 \n",
      "   Insert: |..=.........| V -> 'saw' [0.21]         0.2100000000 \n",
      "   Insert: |..=.........| VP -> V [0.4]             0.0840000000 \n",
      "   Insert: |...=........| Name -> 'Jack' [0.52]     0.5200000000 \n",
      "   Insert: |...=........| NP -> Name [0.28]         0.1456000000 \n",
      "   Insert: |....=.......| P -> 'with' [0.61]        0.6100000000 \n",
      "   Insert: |.....=......| Name -> 'Bob' [0.48]      0.4800000000 \n",
      "   Insert: |.....=......| NP -> Name [0.28]         0.1344000000 \n",
      "   Insert: |......=.....| P -> 'under' [0.39]       0.3900000000 \n",
      "   Insert: |.......=....| Det -> 'the' [0.41]       0.4100000000 \n",
      "   Insert: |........=...| N -> 'table' [0.13]       0.1300000000 \n",
      "   Insert: |.........=..| P -> 'with' [0.61]        0.6100000000 \n",
      "   Insert: |..........=.| Det -> 'a' [0.31]         0.3100000000 \n",
      "   Insert: |...........=| N -> 'telescope' [0.14]   0.1400000000 \n",
      "Finding the most likely constituents spanning 2 text elements...\n",
      "   Insert: |==..........| NP -> Det N [0.41]        0.0184910000 \n",
      "   Insert: |..==........| VP -> V NP [0.59]         0.0180398400 \n",
      "   Insert: |....==......| PP -> P NP [1.0]          0.0819840000 \n",
      "   Insert: |.......==...| NP -> Det N [0.41]        0.0218530000 \n",
      "   Insert: |..........==| NP -> Det N [0.41]        0.0177940000 \n",
      "Finding the most likely constituents spanning 3 text elements...\n",
      "   Insert: |===.........| S -> NP VP [1.0]          0.0015532440 \n",
      "   Insert: |...===......| NP -> NP PP [0.31]        0.0037004298 \n",
      "   Insert: |......===...| PP -> P NP [1.0]          0.0085226700 \n",
      "   Insert: |.........===| PP -> P NP [1.0]          0.0108543400 \n",
      "Finding the most likely constituents spanning 4 text elements...\n",
      "   Insert: |====........| S -> NP VP [1.0]          0.0003335747 \n",
      "   Insert: |..====......| VP -> V NP [0.59]         0.0004584833 \n",
      "  Discard: |..====......| VP -> VP PP [0.01]        0.0000147898 \n",
      "  Discard: |..====......| VP -> VP PP [0.01]        0.0000147898 \n",
      "   Insert: |.....====...| NP -> NP PP [0.31]        0.0003550885 \n",
      "Finding the most likely constituents spanning 5 text elements...\n",
      "   Insert: |....=====...| PP -> P NP [1.0]          0.0002166040 \n",
      "   Insert: |.......=====| NP -> NP PP [0.31]        0.0000735320 \n",
      "Finding the most likely constituents spanning 6 text elements...\n",
      "   Insert: |======......| S -> NP VP [1.0]          0.0000084778 \n",
      "   Insert: |...======...| NP -> NP PP [0.31]        0.0000097766 \n",
      "  Discard: |...======...| NP -> NP PP [0.31]        0.0000097766 \n",
      "  Discard: |...======...| NP -> NP PP [0.31]        0.0000097766 \n",
      "   Insert: |......======| PP -> P NP [1.0]          0.0000286775 \n",
      "Finding the most likely constituents spanning 7 text elements...\n",
      "   Insert: |..=======...| VP -> V NP [0.59]         0.0000012113 \n",
      "  Discard: |..=======...| VP -> VP PP [0.01]        0.0000000391 \n",
      "  Discard: |..=======...| VP -> VP PP [0.01]        0.0000000391 \n",
      "  Discard: |..=======...| VP -> VP PP [0.01]        0.0000000391 \n",
      "  Discard: |..=======...| VP -> VP PP [0.01]        0.0000000391 \n",
      "   Insert: |.....=======| NP -> NP PP [0.31]        0.0000011948 \n",
      "   Insert: |.....=======| NP -> NP PP [0.31]        0.0000011948 \n",
      "  Discard: |.....=======| NP -> NP PP [0.31]        0.0000011948 \n",
      "Finding the most likely constituents spanning 8 text elements...\n",
      "   Insert: |....========| PP -> P NP [1.0]          0.0000007288 \n",
      "Finding the most likely constituents spanning 9 text elements...\n",
      "   Insert: |=========...| S -> NP VP [1.0]          0.0000000224 \n",
      "   Insert: |...=========| NP -> NP PP [0.31]        0.0000000329 \n",
      "  Discard: |...=========| NP -> NP PP [0.31]        0.0000000329 \n",
      "  Discard: |...=========| NP -> NP PP [0.31]        0.0000000329 \n",
      "  Discard: |...=========| NP -> NP PP [0.31]        0.0000000329 \n",
      "  Discard: |...=========| NP -> NP PP [0.31]        0.0000000329 \n",
      "Finding the most likely constituents spanning 10 text elements...\n",
      "   Insert: |..==========| VP -> V NP [0.59]         0.0000000041 \n",
      "  Discard: |..==========| VP -> VP PP [0.01]        0.0000000001 \n",
      "  Discard: |..==========| VP -> VP PP [0.01]        0.0000000001 \n",
      "  Discard: |..==========| VP -> VP PP [0.01]        0.0000000001 \n",
      "  Discard: |..==========| VP -> VP PP [0.01]        0.0000000001 \n",
      "  Discard: |..==========| VP -> VP PP [0.01]        0.0000000001 \n",
      "  Discard: |..==========| VP -> VP PP [0.01]        0.0000000001 \n",
      "Finding the most likely constituents spanning 11 text elements...\n",
      "Finding the most likely constituents spanning 12 text elements...\n",
      "   Insert: |============| S -> NP VP [1.0]          0.0000000001 \n",
      "\n",
      "sentence: the boy saw a cookie under my table\n",
      "parser: <ViterbiParser for <Grammar with 23 productions>>\n",
      "grammar: Grammar with 23 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    VP -> V NP [0.59]\n",
      "    VP -> V [0.4]\n",
      "    VP -> VP PP [0.01]\n",
      "    NP -> Det N [0.41]\n",
      "    NP -> Name [0.28]\n",
      "    NP -> NP PP [0.31]\n",
      "    PP -> P NP [1.0]\n",
      "    V -> 'saw' [0.21]\n",
      "    V -> 'ate' [0.51]\n",
      "    V -> 'ran' [0.28]\n",
      "    N -> 'boy' [0.11]\n",
      "    N -> 'cookie' [0.12]\n",
      "    N -> 'table' [0.13]\n",
      "    N -> 'telescope' [0.14]\n",
      "    N -> 'hill' [0.5]\n",
      "    Name -> 'Jack' [0.52]\n",
      "    Name -> 'Bob' [0.48]\n",
      "    P -> 'with' [0.61]\n",
      "    P -> 'under' [0.39]\n",
      "    Det -> 'the' [0.41]\n",
      "    Det -> 'a' [0.31]\n",
      "    Det -> 'my' [0.28]\n",
      "Inserting tokens into the most likely constituents table...\n",
      "   Insert: |=.......| the\n",
      "   Insert: |.=......| boy\n",
      "   Insert: |..=.....| saw\n",
      "   Insert: |...=....| a\n",
      "   Insert: |....=...| cookie\n",
      "   Insert: |.....=..| under\n",
      "   Insert: |......=.| my\n",
      "   Insert: |.......=| table\n",
      "Finding the most likely constituents spanning 1 text elements...\n",
      "   Insert: |=.......| Det -> 'the' [0.41]           0.4100000000 \n",
      "   Insert: |.=......| N -> 'boy' [0.11]             0.1100000000 \n",
      "   Insert: |..=.....| V -> 'saw' [0.21]             0.2100000000 \n",
      "   Insert: |..=.....| VP -> V [0.4]                 0.0840000000 \n",
      "   Insert: |...=....| Det -> 'a' [0.31]             0.3100000000 \n",
      "   Insert: |....=...| N -> 'cookie' [0.12]          0.1200000000 \n",
      "   Insert: |.....=..| P -> 'under' [0.39]           0.3900000000 \n",
      "   Insert: |......=.| Det -> 'my' [0.28]            0.2800000000 \n",
      "   Insert: |.......=| N -> 'table' [0.13]           0.1300000000 \n",
      "Finding the most likely constituents spanning 2 text elements...\n",
      "   Insert: |==......| NP -> Det N [0.41]            0.0184910000 \n",
      "   Insert: |...==...| NP -> Det N [0.41]            0.0152520000 \n",
      "   Insert: |......==| NP -> Det N [0.41]            0.0149240000 \n",
      "Finding the most likely constituents spanning 3 text elements...\n",
      "   Insert: |===.....| S -> NP VP [1.0]              0.0015532440 \n",
      "   Insert: |..===...| VP -> V NP [0.59]             0.0018897228 \n",
      "   Insert: |.....===| PP -> P NP [1.0]              0.0058203600 \n",
      "Finding the most likely constituents spanning 4 text elements...\n",
      "Finding the most likely constituents spanning 5 text elements...\n",
      "   Insert: |=====...| S -> NP VP [1.0]              0.0000349429 \n",
      "   Insert: |...=====| NP -> NP PP [0.31]            0.0000275194 \n",
      "Finding the most likely constituents spanning 6 text elements...\n",
      "   Insert: |..======| VP -> V NP [0.59]             0.0000034096 \n",
      "  Discard: |..======| VP -> VP PP [0.01]            0.0000001100 \n",
      "  Discard: |..======| VP -> VP PP [0.01]            0.0000001100 \n",
      "Finding the most likely constituents spanning 7 text elements...\n",
      "Finding the most likely constituents spanning 8 text elements...\n",
      "   Insert: |========| S -> NP VP [1.0]              0.0000000630 \n",
      "\n",
      "sentence: the boy saw hill with telescope\n",
      "parser: <ViterbiParser for <Grammar with 23 productions>>\n",
      "grammar: Grammar with 23 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    VP -> V NP [0.59]\n",
      "    VP -> V [0.4]\n",
      "    VP -> VP PP [0.01]\n",
      "    NP -> Det N [0.41]\n",
      "    NP -> Name [0.28]\n",
      "    NP -> NP PP [0.31]\n",
      "    PP -> P NP [1.0]\n",
      "    V -> 'saw' [0.21]\n",
      "    V -> 'ate' [0.51]\n",
      "    V -> 'ran' [0.28]\n",
      "    N -> 'boy' [0.11]\n",
      "    N -> 'cookie' [0.12]\n",
      "    N -> 'table' [0.13]\n",
      "    N -> 'telescope' [0.14]\n",
      "    N -> 'hill' [0.5]\n",
      "    Name -> 'Jack' [0.52]\n",
      "    Name -> 'Bob' [0.48]\n",
      "    P -> 'with' [0.61]\n",
      "    P -> 'under' [0.39]\n",
      "    Det -> 'the' [0.41]\n",
      "    Det -> 'a' [0.31]\n",
      "    Det -> 'my' [0.28]\n",
      "Inserting tokens into the most likely constituents table...\n",
      "   Insert: |=.....| the\n",
      "   Insert: |.=....| boy\n",
      "   Insert: |..=...| saw\n",
      "   Insert: |...=..| hill\n",
      "   Insert: |....=.| with\n",
      "   Insert: |.....=| telescope\n",
      "Finding the most likely constituents spanning 1 text elements...\n",
      "   Insert: |=.....| Det -> 'the' [0.41]             0.4100000000 \n",
      "   Insert: |.=....| N -> 'boy' [0.11]               0.1100000000 \n",
      "   Insert: |..=...| V -> 'saw' [0.21]               0.2100000000 \n",
      "   Insert: |..=...| VP -> V [0.4]                   0.0840000000 \n",
      "   Insert: |...=..| N -> 'hill' [0.5]               0.5000000000 \n",
      "   Insert: |....=.| P -> 'with' [0.61]              0.6100000000 \n",
      "   Insert: |.....=| N -> 'telescope' [0.14]         0.1400000000 \n",
      "Finding the most likely constituents spanning 2 text elements...\n",
      "   Insert: |==....| NP -> Det N [0.41]              0.0184910000 \n",
      "Finding the most likely constituents spanning 3 text elements...\n",
      "   Insert: |===...| S -> NP VP [1.0]                0.0015532440 \n",
      "Finding the most likely constituents spanning 4 text elements...\n",
      "Finding the most likely constituents spanning 5 text elements...\n",
      "Finding the most likely constituents spanning 6 text elements...\n"
     ]
    }
   ],
   "source": [
    "# demo = [('I saw John with my telescope', toy_pcfg1)]\n",
    "# sent, grammar = demos[0]\n",
    "# As we are getting errors when we parse Review column so we couldnt input it.\n",
    "# As per the direction by the instructor taking 3 sample statements each for toy_pcfg1 and toy_pcfg2 \n",
    "# (same was conveyed to us in Canvas)\n",
    "from nltk.parse import ViterbiParser\n",
    "demos = [('I saw the man with my telescope', toy_pcfg1),(\"John saw my man under telescope\", toy_pcfg1),\n",
    "(\" man saw with my telescope\",toy_pcfg1),(\"the boy saw Jack with Bob under the table with a telescope\", toy_pcfg2),        \n",
    "        (\"the boy saw a cookie under my table\",toy_pcfg2),(\"the boy saw hill with telescope\",toy_pcfg2)] \n",
    "\n",
    "for i in range(len(demos)): \n",
    "    sent=demos[i][0]\n",
    "    grammar=demos[i][1]\n",
    "    tokens = sent.split() \n",
    "    parser = ViterbiParser(grammar) \n",
    "    all_parses = {} \n",
    "    print('\\nsentence: %s\\nparser: %s\\ngrammar: %s' % (sent,parser,grammar))\n",
    "    parser.trace(3)\n",
    "    t = time.time()\n",
    "    parses = parser.parse_all(tokens)\n",
    "    times.append(time.time()-t)\n",
    "    if parses: \n",
    "        lp = len(parses)\n",
    "        p = reduce(lambda a,b:a+b.prob(), parses, 0.0)\n",
    "    else: \n",
    "        p = 0\n",
    "    average_p.append(p)\n",
    "    num_parses.append(len(parses))\n",
    "    for p in parses: \n",
    "        all_parses[p.freeze()] = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
